{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33gkNh0FUXNw"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pneqawo5UXN5"
      },
      "outputs": [],
      "source": [
        "NAME = \"\"\n",
        "COLLABORATORS = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XZh4hYvUXON"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IFkG03QUXOP"
      },
      "source": [
        "# Challenge 5B: Solving $A\\vec{x} = \\lambda \\vec{x}$\n",
        "\n",
        "> _Linear relationships can be captured with a straight line on a graph. Linear relationships are easy to think about.... Linear systems have an important modular virtue: you can take them apart, and put them together again — the pieces add up._\n",
        ">\n",
        "> — James Gleick in _Chaos: Making a New Science_ (1987)\n",
        "\n",
        "The first part of this challenge should have given you a feel for how people turn the cranks and make matrices do useful stuff in practice (and by 'useful' I mean solving equations—you use equations in your daily life, right?)\n",
        "\n",
        "This part is about the theory.\n",
        "\n",
        "So much of what makes modern-day machine learning work won't make sense if you don't have a systematic understanding of what it means for something to change _linearly_: that is, if you double the input, you double the output. No hidden suprises.\n",
        "\n",
        "In some sense a linear transformation is the simplest nontrivial kind of response your system can have (as opposed to trivial ones like zero or constant responses), and it's no accident that we have collectively decided to make it the bedrock upon which we understand all other kinds of change.\n",
        "\n",
        "The behaviour of linear transformations only becomes clearer when we consider what they _preserve_ in the course of morphing spaces, and so by studying them in this challenge you should:\n",
        "\n",
        "* [ ] know what a vector space is, and what having such a structure implies about the stuff you can and cannot do\n",
        "* [ ] get into the habit of seeing beyond individual vectors and looking for their 'completions'; what this means will hopefully become clearer later on\n",
        "* [ ] be able to reformulate a thorny transformation into components that are easier to work with\n",
        "\n",
        "In particular, the main star of the show is the matrix equation $A\\vec{x} = \\lambda \\vec{x}$, and you may think of everything else that follows as mere stepping stones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KeyLFg-UXOR"
      },
      "source": [
        "## Definition: (vector space)\n",
        "\n",
        "A **vector space** or **linear space** $V$ is a set of vectors where the following properties are always satisfied:\n",
        "\n",
        "1. [additive identity] The zero vector $\\vec{0}$ is in $V$; or in other words there's a vector $\\vec{0}$ in $V$ such that $\\vec{0} + \\vec{v} = \\vec{0}$ is always true.\n",
        "2. [additive inverse] Any vector $\\vec{v}$ in $V$ has a corresponding inverse vector $\\vec{-v}$ so that $\\vec{v} + (\\vec{-v}) = \\vec{0}$.\n",
        "3. [addition is commutative] $\\vec{v} + \\vec{w} = \\vec{v} + \\vec{w}$ for all $\\vec{v}, \\vec{w} \\in V$.\n",
        "4. [addition is associative] $\\vec{v_1} + (\\vec{v_2} + \\vec{v_3}) = (\\vec{v_1} + \\vec{v}) + \\vec{v}$ for all $\\vec{v_1}, \\vec{v_2}, \\vec{v_3} \\in V$.\n",
        "5. [multiplicative identity] For all $\\vec{v}$ in $V$, we have $1 \\vec{v} = \\vec{v}$.\n",
        "6. [multiplication is associative] $a(b\\vec{v}) = (ab)\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "7. [scalar addition is distributive] $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "8. [vector addition is distributive] $a(\\vec{v} + \\vec{w}) = a\\vec{v} + a\\vec{w}$ for all scalars $a$ and $\\vec{v}, \\vec{w} \\in V$.\n",
        "\n",
        "---\n",
        "\n",
        "Don't fret: by this point every single one of these properties should already be familiar to you: we're just consolidating them into one list.\n",
        "\n",
        "Also, this is what it really means for a _vector_ to be a _vector_. It's not about having magnitude or direction but about being an element of a _vector space_, obeying all the properties above and interacting with other vectors in precisely the ways we have listed.\n",
        "\n",
        "This also means that there are 'vectors' that are not necessarily floating arrows in space, as the following example will show.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXzRGTCsUXOS"
      },
      "source": [
        "## Example: Light switches\n",
        "\n",
        "Consider $n$ light switches, each of which can take either two states: ON or OFF. Form the set $V$ of possible _configurations_ these light switches can be in, so for example:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\begin{bmatrix} ON \\\\ ON \\\\ \\dots \\\\ ON \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "is one such configuration.\n",
        "\n",
        "Now, let's say 'adding' two configurations means, for each corresponding pair of switch entries, you leave it ON if there is _exactly_ one switch entry in the pair that's ON. Example:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Furthermore, 'multiplying' a configuration by ON means _leaving ON_ only the switches that were already ON, and multiplying by OFF means turning OFF everything. So:\n",
        "\n",
        "$$\n",
        "(ON)\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "(OFF)\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then $V$ is a vector space.\n",
        "\n",
        "---\n",
        "\n",
        "You might also remember the notion of a **vector subspace** from the first session. We can now restate its definition more compactly: it is a subset of a vector space $V$ which is also a vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDTJdaNMUXOY"
      },
      "source": [
        "## Problem 1: Show that $V$ in the light switch example is a vector space.\n",
        "\n",
        "Note that your only scalars are in the set $\\{ON, OFF\\}$.\n",
        "\n",
        "\n",
        "**Solution**. Let $V$ be the light switch vector space and let $v, w \\in V$ be a light switch vector. If $V$ is a vector space, then $v$ and $w$ should satisfy the vector space axioms. It can be observed that the defined vector space is isomorphic to the space of binary addition (base 2 addition). Letting OFF be 0 and ON be 1, it can be directly observed that\n",
        "\n",
        "$$\n",
        "1 + 1 = 0\\\\\n",
        "1 + 0 = 0 + 1 = 1\\\\\n",
        "0 + 0 = 0\n",
        "$$\n",
        "\n",
        "We can further define a zero light switch vector $\\vec{0}$ such that $\\forall$ $i$, $0_i$ = 0. Then,\n",
        "\n",
        "1. [additive identity]\n",
        "\n",
        "Since $1 + 0 = 0 + 1 = 0 + 0$ via the defined addition rules of the vector space, $\\forall i, v_i + 0 = v_i$. Adding 0 to any components of $\\vec{v}\\in V$ does not affect the components. Hence, the vector with all zeroes (defined as the additive identity $\\vec{0}$) does not affect the vector via addition. In short, $\\vec{0}$ is an additive identity $\\vec{v} + \\vec{0} = \\vec{v}$\n",
        "\n",
        "2. [additive inverse]\n",
        "\n",
        "**Claim:** The inverse of $\\vec{v}$ is itself.\n",
        "\n",
        "**Proof**.\n",
        "\n",
        "Let $\\vec{v}$ be an element of the light switch vector space $V$. Then, the components of the addition $\\vec{s} = \\vec{v} + \\vec{v}$ is given by\n",
        "\n",
        "$$\n",
        "s_i = v_i + v_i\n",
        "$$\n",
        "\n",
        "Since $v_i\\in\\{1,0\\}$, we have two cases, either $v_i = 0$ or $v_i = 0$. If $v_i = 1$, $v_i + v_i = 1 + 1 = 0$. If $v_i = 0$, $v_i + v_i = 0 + 0 = 0$. In both cases, $v_i = 0$, $\\forall$ i. Hence, since all the components of $\\vec{s}$ is zero, $\\vec{s} = \\vec{0}$.\n",
        "\n",
        "Hence, $\\vec{v} + \\vec{v} = \\vec{0}$ thus proving that the inverse of $\\vec{v}$ is itself. QED.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. [addition is commutative]\n",
        "\n",
        "Let $\\vec{v}$ and $\\vec{w}$ be elements of the light switch vector space $V$. Then, adding the two vectors $\\vec{s} = \\vec{v} + \\vec{w}$ amounts to adding their components with $s_i = v_i + w_i$ following the above stated binary addition rules.\n",
        "\n",
        "\n",
        "$$\n",
        "1 + 1 = 0\\\\\n",
        "1 + 0 = 0 + 1 = 1\\\\\n",
        "0 + 0 = 0\n",
        "$$\n",
        "\n",
        "Observe that the rules are symmetric and order-independent. Since the rules of the components addition are order-independent, performing addition component-wise must be commutative. Since the addition rules are done component-wise such that $s_i = v_i + w_i$, the vector addition must be commutative. QED.\n",
        "\n",
        "\n",
        "4. [addition is associative]\n",
        "\n",
        "Also, since the vector addition is done component wise, we can also focus on the property of adding up components via the defined addition rules. Again,\n",
        "\n",
        "$$\n",
        "1 + 1 = 0\\\\\n",
        "1 + 0 = 0 + 1 = 1\\\\\n",
        "0 + 0 = 0\n",
        "$$\n",
        "\n",
        "\n",
        "We can prove it similarly. However, we can directly invoke the fact the binary operations are asosciative. Since the defined rules can be equally mapped to executing binary operations, the addition rules on individual components must also be associative. Again, since the vector addition rules are defined via the operations on indiviudal components, the vector addition rule must be associative. QED.\n",
        "\n",
        "\n",
        "\n",
        "5. [multiplicative identity]\n",
        "\n",
        "The defined multiplication rules can be summarized as follows:\n",
        "\n",
        "$$\n",
        "1*1 = 1\\\\\n",
        "1*0 = 0*1 = 0*0 = 0\n",
        "$$\n",
        "\n",
        "\n",
        "It directly follows that multiplying the components by the scalar $1$ does not change the components. That is,\n",
        "\n",
        "$$\n",
        "1 * v_i = v_i\n",
        "$$\n",
        "\n",
        "Hence, $1$ is a multiplicative identity of the light switch vector space. QED\n",
        "\n",
        "\n",
        "\n",
        "6. [multiplication is associative] $a(b\\vec{v}) = (ab)\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "\n",
        "\n",
        "Since there exists only two scalar values in the vector space, we can exhaust scenarios in a proof by cases.\n",
        "\n",
        "**Case 1: a = 1, b= 0**\n",
        "$$\n",
        "a(b\\vec{v}) =  1*(\\vec{0}) = \\vec{0}\\\\\n",
        "(ab)\\vec{v} = 0*(\\vec{v}) = \\vec{0}\n",
        "\\iff a(b\\vec{v}) = (ab)\\vec{0}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Case 1: a = 1, b = 1**\n",
        "$$\n",
        "a(b\\vec{v}) =  1*(\\vec{0}) = \\vec{0}\\\\\n",
        "(ab)\\vec{v} = 0*(\\vec{v}) = \\vec{0}\n",
        "\\iff a(b\\vec{v}) = (ab)\\vec{0}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Case 1: a = 0, b= 0**\n",
        "$$\n",
        "a(b\\vec{v}) =  0*(\\vec{0}) = \\vec{0}\\\\\n",
        "(ab)\\vec{v} = 0*(\\vec{v}) = \\vec{0}\n",
        "\\iff a(b\\vec{v}) = (ab)\\vec{0}\n",
        "$$\n",
        "\n",
        "\n",
        "**Case 1: a = 0 , b = 1**\n",
        "$$\n",
        "a(b\\vec{v}) =  0*(\\vec{v}) = \\vec{0}\\\\\n",
        "(ab)\\vec{v} = 0*(\\vec{v}) = \\vec{0}\n",
        "\\iff a(b\\vec{v}) = (ab)\\vec{0}\n",
        "$$\n",
        "\n",
        "Since we have exhausted all cases of scalar multiplication and have shown that for cases, scalar multiplication is associative, we can conclude the scalar multiplication is associative for all scalars.\n",
        "\n",
        "\n",
        "\n",
        "7. [scalar addition is distributive] $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "\n",
        "\n",
        "8. [vector addition is distributive] $a(\\vec{v} + \\vec{w}) = a\\vec{v} + a\\vec{w}$ for all scalars $a$ and $\\vec{v}, \\vec{w} \\in V$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-25121b0f9fb8a22b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "SzFyEotfUXOa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-327rLyqUXOa"
      },
      "source": [
        "## Definition: (linear combination)\n",
        "\n",
        "A **linear combination** of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ in a vector space $V$ is simply a vector $\\vec{v}$ in $V$ where:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\sum_{i=1}^n {a_i v_i}\n",
        "$$\n",
        "\n",
        "for scalars $a_i$. In other words, it is a _weighted sum_ of $\\vec{v_1}, \\dots, \\vec{v_n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmAN7J5VUXOb"
      },
      "source": [
        "## Definition: (span)\n",
        "\n",
        "A set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ are said to **span** $V$ if and only if every vector in $V$ can be written as a linear combination of $\\vec{v_1}, \\dots, \\vec{v_n}$.\n",
        "\n",
        "Alternatively, the **span** of $\\vec{v_1}, \\dots, \\vec{v_n}$ is the set $\\text{Span } (\\vec{v_1}, \\dots, \\vec{v_n})$ of all their linear combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8OD8UqUUXOe"
      },
      "source": [
        "## Example: Standard basis vectors\n",
        "\n",
        "The standard basis vectors $\\vec{e_1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\vec{e_2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ span $\\mathbb{R}^2$. Why? Because you can write any vector $\\vec{v}$ in $\\mathbb{R}^2$ as:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = v_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + v_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr6rY5iQUXOk"
      },
      "source": [
        "## Definition: (linear independence)\n",
        "\n",
        "A set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ are said to be **linearly independent** if none of $\\vec{v_i}$ can be written as a linear combination of the others.\n",
        "\n",
        "Equivalently, they are linearly independent if the only solution to\n",
        "\n",
        "$$\n",
        "a_1 \\vec{v_1} + \\dots + a_n \\vec{v_n} = \\vec{0}\n",
        "$$\n",
        "\n",
        "is for all the $a_i$ to be equal to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsEnuUJaUXOl"
      },
      "source": [
        "## Example: Standard basis vectors are linearly independent\n",
        "\n",
        "The standard basis vectors in $\\mathbb{R}^3$: $\\vec{e_1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\vec{e_2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $\\vec{e_3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "are linearly independent. To see this, form:\n",
        "\n",
        "$$\n",
        "a_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
        "+\n",
        "a_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "+\n",
        "a_3 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "from which it is clear that $a_1 = a_2 = a_3 = 0$.\n",
        "\n",
        "---\n",
        "\n",
        "To make our lives easier, we shall denote the set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ as $\\{ \\vec{v_i} \\}_{i=1}^n$ or simply $\\{\\vec{v_i}\\}$ if the number of elements is clear from context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9mBPyvIUXOn"
      },
      "source": [
        "## Proposition I: Linear independence and span\n",
        "\n",
        "Let $\\{\\vec{v_i}\\}_{i=1}^k$ be vectors in $\\mathbb{R}^n$. Form the $n \\times k$ matrix $A = [\\vec{v_1}, \\dots, \\vec{v_k}]$ whose ith column is simply $\\vec{v_i}$. Then:\n",
        "\n",
        "1. $\\{\\vec{v_i}\\}$ are linearly independent if the row-reduced matrix $\\tilde{A}$ has a pivotal 1 in every column.\n",
        "2. $\\{\\vec{v_i}\\}$ span $\\mathbb{R}^n$ if and only if $\\tilde{A}$ has a pivotal 1 in every row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-0k7T0fUXOu"
      },
      "source": [
        "## Problem 2: Using Proposition I\n",
        "\n",
        "a. Given $\\vec{w_1} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\vec{w_2} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix}$, $\\vec{w_3} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 2 \\end{bmatrix}$, and $\\vec{v} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "Is $\\vec{v}$ in the span of $\\{w_i\\}$? Check this using row reduction.\n",
        "\n",
        "b. Given $\\vec{w_1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $\\vec{w_2} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 2 \\end{bmatrix}$, $\\vec{w_3} = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}$\n",
        "\n",
        "Are they linearly independent? Do they span $\\mathbb{R}^3$? Check these using row reduction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-84b14f0d1ba32876",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x96vtEXfUXOv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOuWtaSuUXOv"
      },
      "source": [
        "## Problem (bonus): Prove Proposition I\n",
        "\n",
        "<details>\n",
        "<summary>Hint</summary>\n",
        "The theorem you need is in Challenge 5A.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e25962cf8121213d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "vlg1pLtjUXPA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZNy45WQUXPA"
      },
      "source": [
        "## Definition: (basis)\n",
        "\n",
        "A set $\\{\\vec{v_i}\\}$ of vectors in $V$ is called a **basis** of $V$ if _any_ of the following conditions hold:\n",
        "\n",
        "1. The set is a maximally linearly independent set: it is linearly independent, and if you add one more vector, it will no longer be.\n",
        "2. The set is a minimally spanning set: it spans $V$, and if you drop one vector, it will no longer span $V$.\n",
        "3. The set is a linearly independent set spanning $V$.\n",
        "\n",
        "---\n",
        "\n",
        "Choosing a basis for a vector space is in some sense, choosing its _axes_. Each basis vector represents an axis and its magnitude determines the axis' unit of length.\n",
        "\n",
        "Also, it can be shown that all bases for a vector space $V$ will have the same number of elements, which we call the **dimension** of $V$ or $\\text{dim }V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYzrw3PBUXPB"
      },
      "source": [
        "## Example: Standard basis\n",
        "\n",
        "We now understand why $\\{\\vec{e_i}\\}_{i=1}^n$ are called _basis_ vectors: because they form a basis of $\\mathbb{R}^n$.\n",
        "\n",
        "To see this, verify that they span $\\mathbb{R}^n$ and are linearly independent. So by Proposition I, they must serve as a basis (aka the **standard basis**) for $\\mathbb{R}^n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGW2SF_uUXPC"
      },
      "source": [
        "**Q**: Is $\\vec{e} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ part of $\\mathbb{R}^2$?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "No, because it has three entries.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "Point is, there's a different standard basis for every $k$ in $\\mathbb{R}^k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxGO8jlTUXPC"
      },
      "source": [
        "## Problem 3: Find a basis for $\\mathbb{R}^2$ that isn't the standard basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-cd81f2188836489d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aKAIYMJ7UXPC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ii9F-LUXPD"
      },
      "source": [
        "## Definition: (orthonormal basis)\n",
        "\n",
        "A set of vectors $\\{\\vec{v_i}\\}$ is **orthonormal** if each vector is orthogonal to every other one in the set, and if all of them are of length 1. That is:\n",
        "\n",
        "$$\n",
        "\\vec{v_i} \\cdot \\vec{v_j} = 0\n",
        "$$\n",
        "\n",
        "when $i \\neq j$, and $|\\vec{v_k}| = 1$ for all $\\vec{v_k}$.\n",
        "\n",
        "These vectors then are said to form an **orthonormal basis** for $\\text{Span } \\{\\vec{v_i}\\}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDPLCD2TUXPE"
      },
      "source": [
        "## Example: The standard basis is orthonormal\n",
        "\n",
        "We know that $\\{\\vec{e_i}_{i=1}^n\\}$ is a basis for $\\mathbb{R}^n$, and they are clearly of length 1. To show then that it is an orthonormal basis for $\\mathbb{R}^n$, it is enough to verify that they are all orthogonal to each other.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL4Gb65mUXPE"
      },
      "source": [
        "## Definition: (orthogonal matrix)\n",
        "\n",
        "An $n \\times n$ matrix $A$ is **orthogonal** if it satisfies any of the following conditions:\n",
        "\n",
        "1. $A A^T = A^T A = I$, or in other words its transpose is its inverse: $A^T = A^{-1}$.\n",
        "2. The columns of $A$ form an orthonormal basis of $\\mathbb{R}^n$.\n",
        "3. For any $\\vec{v}, \\vec{w} \\in \\mathbb{R}^n$,\n",
        "\n",
        "$$\n",
        "A\\vec{v} \\cdot A\\vec{w} = \\vec{v} \\cdot \\vec{w}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHCQvVZfUXPE"
      },
      "source": [
        "## Definition: (kernel and image)\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation.\n",
        "\n",
        "1. The **kernel** of $T$ or $\\text{ker } T$ is the set of vectors $\\vec{x} \\in \\mathbb{R}^n$ such that $T(\\vec{x}) = \\vec{0}$.\n",
        "2. The **image** of $T$ or $\\text{img } T$ is the set of vectors $\\vec{w} \\in \\mathbb{R}^m$ such that there exists a vector $\\vec{v} \\in \\mathbb{R}^n$ with $T(\\vec{v}) = \\vec{w}$\n",
        "\n",
        "---\n",
        "\n",
        "In other words, the _kernel_ tells you all the vectors a linear transformation sends to $\\vec{0}$, while the _image_ is the set of vectors the transformation can possibly output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT3xy8hNUXPE"
      },
      "source": [
        "## Example: Vector in a kernel\n",
        "\n",
        "Let $A = \\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{bmatrix}$ be (the matrix that represents) a linear transformation. Then $\\vec{v} = \\begin{bmatrix} -2 \\\\ -1 \\\\ 3 \\end{bmatrix}$ is in $\\text{ker } A$, because:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{bmatrix}\n",
        "\\begin{bmatrix} -2 \\\\ -1 \\\\ 3 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "If $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a linear transformation, then $\\text{ker }T$ is a vector subspace of $\\mathbb{R}^n$ and $\\text{img }T$ is a vector subspace of $\\mathbb{R}^m$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUc5u1rmUXPF"
      },
      "source": [
        "## Proposition II: Kernel, image, and solutions to systems of equations\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation. Then the system of (linear) equations $T(\\vec{x}) = \\vec{b}$ has:\n",
        "\n",
        "1. at _most_ one solution for every $\\vec{b} \\in \\mathbb{R}^n$ if and only if $\\text{ker }T = \\{\\vec{0}\\}$\n",
        "\n",
        "2. at _least_ one solution for every $\\vec{b} \\in \\mathbb{R}^n$ if and only if $\\text{img }T = \\mathbb{R}^m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT4F7yxeUXPF"
      },
      "source": [
        "## Problem (bonus): Prove Proposition II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-02924f67aa5b16c5",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "W35S570EUXPF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOCkldLvUXPG"
      },
      "source": [
        "## Proposition III: Dimension and rank\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation. Then:\n",
        "\n",
        "$$\n",
        "\\text{dim }(\\text{ker }T) + \\text{dim }(\\text{img }T) = n\n",
        "$$\n",
        "\n",
        "(the dimension of $\\mathbb{R}^n$)\n",
        "\n",
        "Furthermore, $\\text{dim }(\\text{img }T)$ is called the **rank** of T.\n",
        "\n",
        "---\n",
        "\n",
        "It can be shown by a somewhat involved proof that, if $\\tilde{T}$ is the row-reduced form of the matrix $[T]$ representing $T$, then $\\text{dim }(\\text{img }T)$ is equal to the number of pivotal columns, while $\\text{dim }(\\text{ker }T)$ is equal to the non-pivotal columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxgDnhZjUXPG"
      },
      "source": [
        "## Example: Fibonacci sequence\n",
        "\n",
        "The Fibonacci numbers $1, 1, 2, 3, 5, 8, 13, \\dots$ are usually defined by the following procedure: take the last two numbers and then add them together to get the next one. In equation form:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "a_0 = a_1 &= 1 \\\\\n",
        "a_{n+1} &= a_n + a_{n-1}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "or in matrix form:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_n \\\\\n",
        "a_{n+1}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_{n-1} \\\\\n",
        "a_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Is there a better way of generating the sequence? If we try to do the matrix multiplication repeatedly, we'll find that:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_n \\\\\n",
        "a_{n+1}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}^n\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which looks beautiful but then if you actually try to multiply this out, you realise you're still performing the same calculations. Are we out of luck?\n",
        "\n",
        "Of course not. Let:\n",
        "\n",
        "$$\n",
        "P =\n",
        "\\begin{bmatrix}\n",
        "2 & 2 \\\\\n",
        "1 + \\sqrt{5} & 1 - \\sqrt{5}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which means\n",
        "\n",
        "$$\n",
        "P^{-1} = \\frac{1}{4\\sqrt{5}}\n",
        "\\begin{bmatrix}\n",
        "\\sqrt{5} - 1 & 2 \\\\\n",
        "\\sqrt{5} + 1 & -2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Now, if $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$, then we can write:\n",
        "\n",
        "$$\n",
        "P^{-1} A P =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1 + \\sqrt{5}}{2} & 0 \\\\\n",
        "0 & \\frac{1 - \\sqrt{5}}{2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This is a _diagonal_ matrix! And we know from a previous session that powers of diagonal matrices just square the entries in the diagonal. Hence:\n",
        "\n",
        "$$\n",
        "(P^{-1} A P)^n = (P^{-1} A P)(P^{-1} A P)\\dots(P^{-1} A P)\n",
        "= (P^{-1} A^n P)\n",
        "$$\n",
        "\n",
        "since all the $P P^{-1} = P^{-1} P = I$.\n",
        "\n",
        "Solving for $A^n$ then by multiplying $P$'s and $P^{-1}$'s, we get:\n",
        "\n",
        "$$\n",
        "A^n = P \\, (P^{-1} A P)^n \\, P^{-1}\n",
        "$$\n",
        "\n",
        "And expanding this out by substituting the values (and letting $\\lambda_1 = \\tfrac{1+\\sqrt{5}}{2}$, $\\lambda_2 = \\tfrac{1-\\sqrt{5}}{2}$), we obtain:\n",
        "\n",
        "$$\n",
        "A^n = \\frac{1}{2\\sqrt{5}}\n",
        "\\begin{bmatrix}\n",
        "\\lambda_1^n (\\sqrt{5} - 1) + \\lambda_2^n (\\sqrt{5} + 1) &\n",
        "2\\lambda_1^n - 2\\lambda_2^n \\\\\n",
        "\\lambda_1^{n+1} (\\sqrt{5} - 1) + \\lambda_2^{n+1} (\\sqrt{5} + 1) &\n",
        "2\\lambda_1^{n+1} - 2\\lambda_2^{n+1} \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}^n\n",
        "$$\n",
        "\n",
        "And so by our original matrix equation (the one with $A^n$), we can obtain a _direct_ formula for the nth Fibonacci number $a_n$:\n",
        "\n",
        "$$\n",
        "a_n = \\left( \\frac{5 + \\sqrt{5}}{10} \\lambda_1^n \\right) + \\left( \\frac{5 - \\sqrt{5}}{10} \\lambda_2^n \\right)\n",
        "$$\n",
        "\n",
        "Isn't this remarkable? We only need to calculate $\\lambda_1$ and $\\lambda_2$ now to get to _any_ Fibonacci number, no need to bother unrolling the recurrence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0oIjxxUXPI"
      },
      "source": [
        "## Definition: (eigenvector, eigenvalue, multiplicity)\n",
        "\n",
        "Let $V$ be a vector space and $T: V \\to V$ be a linear transformation (here, we blur the notation between $T$ the transformation and its corresponding matrix $T$).\n",
        "\n",
        "Then a _nonzero_ vector $\\vec{v}$ such that:\n",
        "\n",
        "$$\n",
        "T{\\vec{v}} = \\lambda \\vec{v}\n",
        "$$\n",
        "\n",
        "for some scalar $\\lambda$ is called an **eigenvector** of $T$, and $\\lambda$ is its corresponding **eigenvalue**. The dimension of the set $\\{\\vec{v} | T{\\vec{v}} = \\lambda \\vec{v}\\}$, called the **eigenspace**, is called the **multiplicity** of $\\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "In other words, we are now talking about the matrix equation $A\\vec{x} = \\lambda \\vec{x}$.\n",
        "\n",
        "What this set up tells us is that we are interested in finding the vectors for which applying $T$ results in at most a scaling of said vectors. In some sense, these _eigenvectors_ represent those vectors that are merely stretched, never rotated or sheared. They are directions preserved by $T$, and so knowing what they are goes a long way towards characterising what $T$ really does.\n",
        "\n",
        "Also note that $\\lambda$ may be a complex number.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bgg_YcWUXPI"
      },
      "source": [
        "![img](https://i.postimg.cc/dQxjTGjx/shear.png)\n",
        "\n",
        "_Fig. 1: In this image (from [TreyGreer62](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#/media/File:Mona_Lisa_eigenvector_grid.png) of Wikipedia), the red arrow is sheared by the transformation but the blue arrow stays in the same direction. Therefore, it is an eigenvector._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOqG43XUUXPJ"
      },
      "source": [
        "## Problem 4: Eigenproblems\n",
        "\n",
        "a. Explain why only square matrices can have eigenvectors.\n",
        "\n",
        "b. Show that if $T{\\vec{v}} = \\lambda \\vec{v}$, then $T^k{\\vec{v}} = \\lambda^k \\vec{v}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d56bbabb03b3ba5b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "p4lACgXuUXPJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Xox_WVUXPJ"
      },
      "source": [
        "## Definition: (eigenbasis)\n",
        "\n",
        "A basis for a vector space $V$ is an **eigenbasis** of $V$ for a linear transformation $T$ if each element of the basis is an eigenvector of $T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE1mvNDyUXPJ"
      },
      "source": [
        "## Problem 5: Demystifying the Fibonacci formula\n",
        "\n",
        "a. Verify that the columns of $P$ in the example above form an eigenbasis of $\\mathbb{R}^2$ for the linear transformation represented by $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$\n",
        "\n",
        "b. Verify that the diagonals $\\lambda_1$ and $\\lambda_2$ of $P^{-1} A P$ are the corresponding eigenvalues of the eigenvectors above.\n",
        "\n",
        "c. Plot the first eigenvector $\\begin{bmatrix} 2 \\\\ 1 + \\sqrt{5} \\end{bmatrix}$ of $P$ and the first 10 Fibonacci vectors $\\begin{bmatrix} a_n \\\\ a_{n+1} \\end{bmatrix}$. How are they related?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3acdc96b4cc1f3ae",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "RrDPFLGRUXPK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLVifCmeUXPK"
      },
      "source": [
        "## Definition: (diagonalizable matrix)\n",
        "\n",
        "A matrix $A$ is **diagonalisable** if there exists a matrix $P$ such that $P^{-1} A P$ is diagonal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL3UmcyuUXPL"
      },
      "source": [
        "## Proposition IV: Diagonalization\n",
        "\n",
        "Let $A$ be an $n \\times n$ matrix and $P = [\\vec{v_1}, \\dots, \\vec{v_n}]$ an invertible $n \\times n$ matrix.\n",
        "\n",
        "Then the following are true:\n",
        "\n",
        "1. The eigenvalues of $A$ and the eigenvalues of $P^{-1} A P$ coincide.\n",
        "2. If $P^{-1} A P$ is a diagonal matrix with diagonal entries $\\lambda_i$, the columns of P are eigenvectors of $A$, with eigenvalues $\\lambda_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-P16pPHUXPL"
      },
      "source": [
        "## Problem (bonus): Prove Proposition IV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-659d4fb3a9c9596e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7NGO_k4sUXPL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobfeW0_UXPM"
      },
      "source": [
        "## Proposition V: Eigenvectors with distinct eigenvalues are linearly independent\n",
        "\n",
        "If $A: V \\to V$ is a linear transformation and $\\{\\vec{v_i}\\}$ are eigenvectors of $A$ with distinct eigenvalues $\\{\\lambda_i\\}$, then $\\{\\vec{v_i}\\}$ are linearly independent.\n",
        "\n",
        "---\n",
        "\n",
        "In particular, there are at most $\\text{dim }V = n$ eigenvalues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX3j59FrUXPM"
      },
      "source": [
        "## Problem 6: Prove Proposition V by contradiction.\n",
        "\n",
        "Hint: you will need to use $\\lambda_j I - A$, where $0 \\leq j \\leq i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-14546b21b2948958",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NB5MhudjUXPN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohItpBvsUXPO"
      },
      "source": [
        "---\n",
        "\n",
        "So what have we done? We've seen that to study a linear transformation, it is enough to look at its eigenbasis, and furthermore, that there is a process called _diagonalisation_ that simplifies certain calculations involving a transformation without changing what it is doing.\n",
        "\n",
        "This was particularly salient in the Fibonacci problem when we constructed $P$ from the eigenvectors of $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$ and arrived at a closed-form formula for $a_n$ after forming $P^{-1} A P$.\n",
        "\n",
        "To understand what an eigenbasis meant, it was necessary for us to understand how and when we can combine vectors and what kind of mathematical objects we can build from them.\n",
        "\n",
        "But how do we find the eigenbasis for a linear transformation in the first place? And how does that help us understand neural networks? Stay tuned for more. 🙂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSNl-mkAUXPP"
      },
      "source": [
        "## Additional resources\n",
        "\n",
        "* _The Essence of Linear Algebra_, by 3Blue1Brown: [LINK](https://www.3blue1brown.com/topics/linear-algebra) (2-3 hours)\n",
        "* _Orthogonal Vectors and Subspaces_, an MIT OpenCourseWare lecture video by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v2:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/587d3844f821441d94418db8b125357e/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40587d3844f821441d94418db8b125357e) (~50 mins)\n",
        "* _Eigenvalues and Eigenvectors_, by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/ff5e10d34f074173a7ace8d54d0f5238/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40ff5e10d34f074173a7ace8d54d0f5238) (~50 mins)\n",
        "* _Diagonalization and Powers of A_, by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/a5d0b53714b646839c528115ef3157b7/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40a5d0b53714b646839c528115ef3157b7) (~50 mins)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}